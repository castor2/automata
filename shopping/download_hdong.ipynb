{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9445b0-5580-42d0-8a0e-2c5744eb41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import codecs\n",
    "import urllib\n",
    "import wget\n",
    "import re    #regular expression \n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import zipfile\n",
    "from tabulate import tabulate\n",
    "\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "\n",
    "\n",
    "conf = yaml.load(open('download_config.yaml'), Loader=yaml.FullLoader)\n",
    "\n",
    "url_in  = conf['site_info']['url_hdong']\n",
    "url_dns = urlparse(url_in).scheme + \"://\" + urlparse(url_in).netloc\n",
    "path_data_hdong = conf['site_info']['path_data_hdong']\n",
    "\n",
    "SLACK_TOKEN = conf['slack_info']['token']\n",
    "V2N_MONITORING_CHANNEL = conf['slack_info']['channel']\n",
    "client = WebClient(token=SLACK_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ccfb3-84af-4c9e-9a2b-aab2c334e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_slack_msg(slack_msg):\n",
    "    try:\n",
    "        response = client.chat_postMessage(\n",
    "            channel=V2N_MONITORING_CHANNEL,\n",
    "            text=slack_msg\n",
    "        )\n",
    "    except SlackApiError as e:\n",
    "        # You will get a SlackApiError if \"ok\" is False\n",
    "        assert e.response[\"error\"]    # str like 'invalid_auth', 'channel_not_found'\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/40145631/precisely-catch-dns-error-with-python-requests\n",
    "def sitecheck(url):\n",
    "    status = None\n",
    "    message = ''\n",
    "    try:\n",
    "        resp = requests.head('http://' + url)\n",
    "        status = str(resp.status_code)\n",
    "    except requests.ConnectionError as exc:\n",
    "        print(exc)\n",
    "\n",
    "        if (\"[Errno 11001] getaddrinfo failed\" in str(exc) or     # Windows\n",
    "            \"[Errno -2] Name or service not known\" in str(exc) or # Linux\n",
    "            \"[Errno 8] nodename nor servname \" in str(exc)):      # OS X\n",
    "            message = 'DNSLookupError'\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    return url, status, message\n",
    "\n",
    "# ref https://stackoverflow.com/questions/70753206/download-and-extract-only-news-from-bbc\n",
    "def get_links(url):\n",
    "    LINKS = []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "    #print(soup)\n",
    "    #soup = codecs.open(\"test.html\", \"r\", \"utf-8\")\n",
    "    #contents = soup.read()\n",
    "\n",
    "    # get table of links\n",
    "    #for html in soup.find_all('li', class_name='wrap'):\n",
    "    for html in soup.find_all('div', 'wrap'):\n",
    "        link = html.find('a').get('href')\n",
    "        if \"https://\" in link:\n",
    "            LINKS.append(link)\n",
    "        else:\n",
    "            # link = url+link\n",
    "            link = url_dns + link\n",
    "            LINKS.append(link)\n",
    "        print(\"adding \", link)\n",
    "    \n",
    "    #NEW_LINKS = []\n",
    "    #for link in LINKS:\n",
    "    #    # print(link) # https://www.mois.go.kr/frt/bbs/type001/commonSelectBoardList.do?bbsId=BBSMSTR_000000000052/frt/bbs/type001/commonSelectBoardArticle.do;jsessionid=5C2MFy-f8LqD8CjwDP939Fal.node10?bbsId=BBSMSTR_000000000052&nttId=101209\n",
    "    #    parsed_url = urlparse(link)\n",
    "    #    # print(parsed_url.query) # bbsId=BBSMSTR_000000000052/frt/bbs/type001/commonSelectBoardArticle.do;jsessionid=5C2MFy-f8LqD8CjwDP939Fal.node10?bbsId=BBSMSTR_000000000052&nttId=106692\n",
    "    #    bbsId_org = parse_qs(parsed_url.query)['bbsId'] # BBSMSTR_000000000052/frt/bbs/type001/commonSelectBoardArticle.do;jsessionid=EdoTpHz-2hU-SBFcvLXcOZUK.node50?bbsId=BBSMSTR_000000000052\n",
    "    #    bbsId = bbsId_org[0].split('/')[0] # BBSMSTR_000000000052\n",
    "    #    nttId = parse_qs(parsed_url.query)['nttId'][0] # 106692\n",
    "    #    # add links https://www.mois.go.kr/frt/bbs/type001/commonSelectBoardArticle.do?bbsId=BBSMSTR_000000000052&nttId=106692\n",
    "    #    NEW_LINKS.append('https://www.mois.go.kr/frt/bbs/type001/commonSelectBoardArticle.do?'+'bbsId='+bbsId+'&nttId='+nttId)\n",
    "\n",
    "    return LINKS\n",
    "\n",
    "def get_html(url):\n",
    "    LINKS = []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "    #print(soup)\n",
    "    #soup = codecs.open(\"test.html\", \"r\", \"utf-8\")\n",
    "    #contents = soup.read()\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def get_donwload_links(url): \n",
    "    FILES = []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "\n",
    "    # fileList\n",
    "    # [0] jscode20240201.zip\n",
    "    # [1] jscode20240201(말소코드포함).zip\n",
    "    # [2] 240201 행정기관(행정동) 및 관할구역(법정동) 변경내역(예천군 호명읍).hwpx\n",
    "    # [3] 행정기관(행정동) 및 관할구역(법정동) 코드 Layout.hwpx\n",
    "    for html in soup.find_all('div', 'fileList'):\n",
    "        link = html.find('a').get('href') # /cmm/fms/FileDown.do?atchFileId=FILE_00124537hf7rFYg&fileSn=0\n",
    "        if \"https://\" in link:\n",
    "            FILES.append(link)\n",
    "        else:\n",
    "            # link = url+link\n",
    "            link = url_dns + link\n",
    "            FILES.append(link)\n",
    "        print(\"adding download link:\", link)\n",
    "\n",
    "    # print(FILES) # 'https://www.mois.go.kr/cmm/fms/FileDown.do?atchFileId=FILE_00124537hf7rFYg&fileSn=0']\n",
    "\n",
    "    return FILES\n",
    "#\n",
    "# download the first attached file from article link\n",
    "#\n",
    "def download_files(article_link):\n",
    "    FILE_LINKS = get_donwload_links(article_link)\n",
    "    FILE_NAMES = []\n",
    "    FILE_DATES = []\n",
    "    STATUS = 'READY'\n",
    "\n",
    "    # sample https://www.mois.go.kr/cmm/fms/FileDown.do?atchFileId=FILE_00124537hf7rFYg&fileSn=0\n",
    "    for link in FILE_LINKS:\n",
    "        #final_url = requests.head(link, allow_redirects=True).url\n",
    "        #print('final_url:', final_url)\n",
    "\n",
    "        result = wget.download(link)\n",
    "        print('file download: ', result)\n",
    "    \n",
    "        # ref https://stackoverflow.com/questions/12595051/check-if-string-matches-pattern\n",
    "        if re.match(r\"jscode[0-9]{8}.zip\", result):\n",
    "            version_date = re.findall(r'\\d+',result)[0]  # first digit string like 20240201\n",
    "            print('effective date:', version_date)\n",
    "            FILE_NAMES.append(result)\n",
    "            FILE_DATES.append(version_date)\n",
    "            STATUS = 'OK'\n",
    "        elif re.match(r\"jscode[0-9]{8}[0-9() ]{3,10}.zip\", result):\n",
    "            print('delete duplicated file:', result, ', use existing one')\n",
    "            os.remove(result)\n",
    "            version_date = re.findall(r'\\d+',result)[0]\n",
    "            print('effective date:', version_date)\n",
    "            FILE_NAMES.append(result)\n",
    "            FILE_DATES.append(version_date)\n",
    "            STATUS = 'DUPLICATED'\n",
    "        else:\n",
    "            print('not a jscode file. delete:',result)\n",
    "            os.remove(result)\n",
    "            STATUS = 'NOT JSCODE FILE'\n",
    "\n",
    "    return FILE_DATES, FILE_NAMES, STATUS\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfbfe2-05da-4f04-945f-6283f946a927",
   "metadata": {},
   "source": [
    "## unzipped files \n",
    "6190366 Jan 22 18:02 KIKcd_B.20240201\\\n",
    " 762910 Jan 22 18:06 KIKcd_B.20240201.xlsx\\\n",
    "1178415 Jan 22 18:02 KIKcd_H.20240201\\\n",
    " 147134 Jan 22 18:10 KIKcd_H.20240201.xlsx\\\n",
    "3598320 Jan 22 18:02 KIKmix.20240201\\\n",
    " 875062 Jan 22 18:13 KIKmix.20240201.xlsx\n",
    "\n",
    "# csv files\n",
    "KIKcd_B.20240201\\\n",
    "KIKcd_H.20240201\\\n",
    "KIKmix.20240201\n",
    "\n",
    "# xls files \n",
    "KIKcd_B.20240201.xlsx\\\n",
    "KIKcd_H.20240201.xlsx\\\n",
    "KIKmix.20240201.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdcec4e-afd0-460a-b48b-1e53c082a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_csv(date, filename):\n",
    "    unzip_path = path_data_hdong+'/'+date\n",
    "    \n",
    "    if( os.path.exists(path_data_hdong)!=True ): \n",
    "        # make directory data_hdong/20240206 \n",
    "        #os.mkdir(path_data_hdong, mode = 0o755, *, dir_fd = None)\n",
    "        os.mkdir(path_data_hdong)\n",
    "        os.mkdir(unzip_path)\n",
    "    elif( os.path.exists(unzip_path)!=True ): \n",
    "        os.mkdir(unzip_path)\n",
    "    else:\n",
    "        # do nothing\n",
    "        print('directory already exist:', unzip_path)\n",
    "        \n",
    "\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(unzip_path)\n",
    "\n",
    "    #df_hdong = pd.read_csv('data_hdong/KIKcd_H.20240201', encoding='CP949')\n",
    "    #df_bdong = pd.read_csv('data_hdong/KIKcd_B.20240201', encoding='CP949')\n",
    "    #df_mix   = pd.read_csv('data_hdong/KIKmix.20240201', encoding='CP949')\n",
    "    df_hdong = pd.read_excel(unzip_path+'/KIKcd_H.'+ date +'.xlsx')\n",
    "    df_bdong = pd.read_excel(unzip_path+'/KIKcd_B.'+ date +'.xlsx')\n",
    "    df_mix   = pd.read_excel(unzip_path+'/KIKmix.' + date +'.xlsx')\n",
    "    \n",
    "    print('hdong:', df_hdong.shape)\n",
    "    print('bdong:', df_bdong.shape)\n",
    "    print('mix:  ', df_mix.shape)\n",
    "    df_mix.rename(columns={'행정동코드': 'adong_cd', '시도명': 'region', '시군구명': 'sgg', '읍면동명':'emd', '법정동코드':'ldong_cd', '동리명':'emd_l', '생성일자':'createdAt', '말소일자':'deletedAt'},  inplace = True, errors=\"raise\")\n",
    "    df_mix['vaildFrom']=date\n",
    "\n",
    "    csv_filename = unzip_path+'/KIKmix.'+date+'.csv' \n",
    "    df_mix.to_csv(csv_filename, sep=',', na_rep='', index=False)\n",
    "    #check\n",
    "    # df_mix = pd.read_csv('data_hdong/KIKmix.20240201.csv', encoding='utf-8')\n",
    "    # df_mix.head(20)\n",
    "\n",
    "    return csv_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e834806-7f29-42de-9c62-9b782549526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test shell\n",
    "\n",
    "article_links = get_links(url_in)\n",
    "\n",
    "# get latest article [0]\n",
    "dates, names, status = download_files(article_links[0])\n",
    "print(dates, names, status)\n",
    "if (status == 'OK' ): \n",
    "    slack_msg = 'incoming file: ' + names[0] + '\\nvalid from ' + dates[0]\n",
    "    send_slack_msg(slack_msg)\n",
    "    print('save jscode file to csv')\n",
    "    filename = unzip_csv(dates[0], names[0])\n",
    "    send_slack_msg('saved to ' + filename)\n",
    "\n",
    "elif (status == 'DUPLICATED' ): \n",
    "    print('already exists. skip')\n",
    "else: \n",
    "    print('not a jscode file')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
